import json
import os
from typing import List, Dict, Optional
import chromadb
from chromadb.config import Settings
import google.generativeai as genai
from dotenv import load_dotenv
import time
import re
import numpy as np



load_dotenv()

class HybridLegalRetrieval:
    """
    Hybrid retrieval system combining:
    1. AI-based title filtering (like two-step)
    2. ChromaDB filtered RAG (only on relevant laws)
    3. Gemini answer generation
    
    This avoids sending entire files OR searching entire DB.
    """
    
    def __init__(self, 
                 titles_file: str = "titles_only.json",
                 db_path: str = "./chroma_db_gemini",
                 model_name: str = "gemini-2.0-flash-exp"):
        """
        Initialize hybrid retrieval system
        
        Args:
            titles_file: JSON file with law titles for initial filtering
            db_path: Path to ChromaDB database
            model_name: Gemini model to use
        """
        self.titles_file = titles_file
        self.db_path = db_path
        self.model_name = model_name
        
        # Initialize Gemini
        genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
        
        # Load titles database
        self.laws_titles = self._load_laws_data(titles_file)
        print(f"‚úÖ Loaded {len(self.laws_titles)} law titles for filtering")
        
        # Initialize ChromaDB
        self.chroma_client = chromadb.PersistentClient(
            path=db_path,
            settings=Settings(anonymized_telemetry=False)
        )
        
        # Get collection
        try:
            self.collection = self.chroma_client.get_collection(
                name="swedish_legal_documents_gemini"
            )
            print(f"‚úÖ Connected to ChromaDB: {self.collection.count()} total documents")
        except Exception as e:
            raise Exception(f"Failed to connect to ChromaDB: {e}")
    
    def _load_laws_data(self, json_file_path: str) -> List[str]:
        """Load law titles from JSON"""
        try:
            with open(json_file_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            raise Exception(f"Failed to load titles from {json_file_path}: {e}")
    
    def find_relevant_law_titles(self, user_query: str) -> List[str]:
        """
        Step 1: Use Gemini to identify relevant law titles from database
        (Same as two-step retrieval)
        """
        print(f"\nüîç Step 1: Finding relevant law titles for: '{user_query}'")
        
        laws_context = json.dumps(self.laws_titles, ensure_ascii=False, indent=2)
        
        prompt = f"""You are a Swedish legal research assistant. You have access to a database of Swedish laws.

SWEDISH LAWS DATABASE:
{laws_context}

USER QUERY: {user_query}

INSTRUCTIONS:
1. Identify the 1-10 most relevant laws that likely contain the answer
2. Return ONLY a JSON array of exact titles from the database
3. Be precise - use exact titles as they appear

Response format:
[
    "exact title 1",
    "exact title 2",
    ...
]

Return [] if no relevant laws found.
CRITICAL: Return ONLY the JSON array, no explanations."""

        try:
            model = genai.GenerativeModel(self.model_name)
            response = model.generate_content(
                prompt,
                generation_config=genai.types.GenerationConfig(
                    temperature=0.1,
                    max_output_tokens=2048,
                )
            )
            
            assistant_message = response.text.strip()
            
            # Try to parse JSON response
            try:
                result = json.loads(assistant_message)
                print(f"‚úÖ Step 1 complete: Found {len(result)} relevant law titles")
                return result
            except json.JSONDecodeError:
                # Fallback: try to extract JSON from text
                import re
                json_match = re.search(r'\[.*?\]', assistant_message, re.DOTALL)
                if json_match:
                    try:
                        result = json.loads(json_match.group())
                        print(f"‚úÖ Step 1 complete: Found {len(result)} relevant law titles (via fallback)")
                        print(f"   Found titles: {result}")
                        return result
                    except json.JSONDecodeError:
                        print(f"‚ùå Failed to parse JSON from fallback: {json_match.group()}")
                        return []
            
            
        except Exception as e:
            print(f"‚ùå Error in Step 1: {e}")
            return []
    
    def filter_chromadb_by_titles(self, titles: List[str]) -> List[str]:
        """
        Step 2: Get all ChromaDB IDs that match the given titles
        This filters the database to only relevant laws
        """
        print(f"\nüîç Step 2: Filtering ChromaDB by {len(titles)} titles...")
        
        if not titles:
            return []
        
        try:
            # Use ChromaDB's efficient metadata filtering with $in operator
            # This is much faster than retrieving all documents
            matching_ids = []
            
            # ChromaDB supports filtering by metadata using where clause
            where_clause = {"title": {"$in": titles}}
            
            # Get only the IDs of matching documents
            filtered_data = self.collection.get(
                where=where_clause,
                include=[]  # Only get IDs, no content or metadata needed
            )
            
            matching_ids = filtered_data['ids']
            
            print(f"‚úÖ Step 2 complete: Found {len(matching_ids)} matching chunks in ChromaDB")
            print(f"   (From {len(titles)} laws)")
            
            return matching_ids
            
        except Exception as e:
            print(f"‚ùå Error filtering ChromaDB: {e}")
            return []
    
    def get_embedding(self, text: str) -> List[float]:
        """Get query embedding from Gemini"""
        try:
            result = genai.embed_content(
                model="models/embedding-001",
                content=text,
                task_type="retrieval_query"
            )
            return result['embedding']
        except Exception as e:
            raise Exception(f"Failed to get embedding: {e}")
    def _write_debug_chunks(self, results, query):
        """Write debug information about retrieved chunks to a log file."""
        import os
        from datetime import datetime
        
        # Create debug_logs directory if it doesn't exist
        debug_dir = "debug_logs"
        if not os.path.exists(debug_dir):
            os.makedirs(debug_dir)
        
        # Generate timestamp for unique filename
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        debug_file = os.path.join(debug_dir, f"chunks_debug_{timestamp}.txt")
        
        try:
            with open(debug_file, 'w', encoding='utf-8') as f:
                f.write(f"Debug Log - Retrieved Chunks\n")
                f.write(f"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"Query: {query}\n")
                f.write(f"Number of chunks retrieved: {len(results['documents'][0])}\n")
                f.write("=" * 80 + "\n\n")
                
                # Write each chunk's content and metadata
                for i, (doc, metadata, distance) in enumerate(zip(
                    results['documents'][0], 
                    results['metadatas'][0], 
                    results['distances'][0]
                )):
                    f.write(f"CHUNK {i+1}\n")
                    f.write(f"Distance/Similarity Score: {distance:.4f}\n")
                    f.write(f"Title: {metadata.get('title', 'N/A')}\n")
                    f.write(f"URL: {metadata.get('url', 'N/A')}\n")
                    f.write(f"Content Length: {len(doc)} characters\n")
                    f.write("-" * 40 + "\n")
                    f.write(f"Content:\n{doc}\n")
                    f.write("-" * 40 + "\n\n")
            
            print(f"üìù Debug log written to: {debug_file}")
            
        except Exception as e:
            print(f"‚ö†Ô∏è Warning: Could not write debug log: {e}")
    def search_filtered_chunks(self, query: str, filtered_ids: List[str], top_k: int = 10) -> Dict:
        """
        Step 3: Perform semantic search ONLY on filtered chunks
        This is the key optimization - we only search relevant laws!
        """
        print(f"\nüîç Step 3: Performing semantic search on {len(filtered_ids)} filtered chunks...")
        
        if not filtered_ids:
            return {"documents": [[]], "metadatas": [[]], "distances": [[]]}
        
        try:
            # Get query embedding
            query_embedding = self.get_embedding(query)
            
            # Get the filtered documents with their embeddings
            # We need to retrieve in batches if there are too many IDs
            batch_size = 1000  # ChromaDB limit
            all_documents = []
            all_metadatas = []
            all_embeddings = []
            all_ids = []
            
            for i in range(0, len(filtered_ids), batch_size):
                batch_ids = filtered_ids[i:i + batch_size]
                batch_data = self.collection.get(
                    ids=batch_ids,
                    include=["documents", "metadatas", "embeddings"]
                )
                all_documents.extend(batch_data['documents'])
                all_metadatas.extend(batch_data['metadatas'])
                all_embeddings.extend(batch_data['embeddings'])
                all_ids.extend(batch_data['ids'])
            
            # Compute similarities manually
            import numpy as np
            
            query_embedding = np.array(query_embedding)
            similarities = []
            
            for embedding in all_embeddings:
                doc_embedding = np.array(embedding)
                # Compute cosine similarity
                similarity = np.dot(query_embedding, doc_embedding) / (
                    np.linalg.norm(query_embedding) * np.linalg.norm(doc_embedding)
                )
                similarities.append(similarity)
            
            # Sort by similarity (highest first) and get top_k
            sorted_indices = np.argsort(similarities)[::-1][:top_k]
            
            # Format results
            filtered_results = {
                "documents": [[]],
                "metadatas": [[]],
                "distances": [[]]
            }
            
            for idx in sorted_indices:
                filtered_results["documents"][0].append(all_documents[idx])
                filtered_results["metadatas"][0].append(all_metadatas[idx])
                # Convert similarity to distance (ChromaDB uses distance, lower is better)
                distance = 1.0 - similarities[idx]
                filtered_results["distances"][0].append(distance)
            
            print(f"‚úÖ Step 3 complete: Retrieved top {len(filtered_results['documents'][0])} most relevant chunks")
            # Debug logging: Write chunks content to file
            self._write_debug_chunks(filtered_results, query)
            
            return filtered_results
            
        except Exception as e:
            print(f"‚ùå Error during filtered search: {e}")
            return {"documents": [[]], "metadatas": [[]], "distances": [[]]}
    
    def format_context(self, search_results: Dict) -> str:
        """Format search results into context for Gemini"""
        context_parts = []
        
        for i, (doc, metadata) in enumerate(zip(
            search_results['documents'][0],
            search_results['metadatas'][0]
        )):
            chunk_info = ""
            if metadata.get('is_chunked'):
                chunk_info = f" (Del {metadata['chunk_position']})"
            
            context_part = f"""
{'='*80}
DOKUMENT {i+1}:
Titel: {metadata['title']}{chunk_info}
SFS-nummer: {metadata.get('sfs_number', 'N/A')}
Myndighet: {metadata.get('ministry_authority', 'N/A')}

Inneh√•ll:
{doc}

K√§lla: {metadata.get('source_link', 'N/A')}
{'='*80}
"""
            context_parts.append(context_part)
        
        return "\n".join(context_parts)
    
    def generate_answer(self, query: str, context: str) -> str:
        """Step 4: Generate answer using Gemini with filtered context"""
        print(f"\nü§ñ Step 4: Generating answer with filtered context...")
        
        prompt = f"""Du √§r en expert p√• svensk lagstiftning och juridiska dokument. Du har tillg√•ng till texten fr√•n relevanta svenska lagar. Din uppgift √§r att besvara fr√•gor baserat p√• de tillhandah√•llna svenska lagdokumenten, utifr√•n din f√∂rst√•else av texten.

INSTRUKTIONER:
1. L√§s och analysera noggrant den fullst√§ndiga lagtexten som tillhandah√•lls och se till att du f√∂rst√•r den fullt ut.
2. Besvara anv√§ndarens fr√•ga utifr√•n informationen i dessa lagar och din f√∂rst√•else av lagarna.
3. Citera den specifika lagen och paragrafen du refererar till f√∂r att st√∂dja ditt svar.
4. Om svaret inte kan hittas i de tillhandah√•llna lagarna eller din f√∂rst√•else av dem, ange tydligt: "Jag kan inte besvara den fr√•gan."
5. Ge ett klart, omfattande och korrekt svar.
6. Inkludera relevanta citat eller specifika avsnitt fr√•n lagtexten n√§r det √§r hj√§lpsamt.
7. Svara p√• svenska.

FR√ÖGA: {query}

TILLHANDAH√ÖLLNA DOKUMENT:
{context}

SVAR:"""
        
        try:
            model = genai.GenerativeModel('gemini-2.0-flash')
            response = model.generate_content(prompt)
            print("‚úÖ Step 4 complete: Answer generated")
            return response.text
            
        except Exception as e:
            raise Exception(f"Failed to generate answer: {e}")
    
    def process_query(self, query: str, top_k: int = 50) -> Dict:
        """
        Complete hybrid retrieval pipeline:
        1. Find relevant law titles using AI
        2. Filter ChromaDB to only those laws
        3. Perform semantic search on filtered subset
        4. Generate answer from top results
        
        Args:
            query: User's question
            top_k: Number of top chunks to retrieve from filtered set
            
        Returns:
            Dict with answer, sources, and metadata
        """
        print(f"\nüöÄ Starting Hybrid Retrieval Pipeline")
        print(f"üìù Query: '{query}'")
        print("=" * 80)
        
        start_time = time.time()
        
        try:
            # Step 1: Find relevant law titles
            relevant_titles = self.find_relevant_law_titles(query)
            
            if not relevant_titles:
                return {
                    "answer": "Inga relevanta lagar hittades f√∂r din fr√•ga.",
                    "sources": [],
                    "processing_time": time.time() - start_time,
                    "method": "hybrid_filtered_rag",
                    "stats": {
                        "titles_found": 0,
                        "chunks_filtered": 0,
                        "chunks_retrieved": 0
                    }
                }
            
            # Step 2: Filter ChromaDB by titles
            filtered_ids = self.filter_chromadb_by_titles(relevant_titles)
            
            if not filtered_ids:
                return {
                    "answer": "Lagar identifierades men inga matchande dokument hittades i databasen.",
                    "sources": [],
                    "processing_time": time.time() - start_time,
                    "method": "hybrid_filtered_rag",
                    "stats": {
                        "titles_found": len(relevant_titles),
                        "chunks_filtered": 0,
                        "chunks_retrieved": 0
                    }
                }
            
            # Step 3: Semantic search on filtered chunks
            search_results = self.search_filtered_chunks(query, filtered_ids, top_k)
            
            if not search_results['documents'][0]:
                return {
                    "answer": "Inga relevanta avsnitt hittades i de identifierade lagarna.",
                    "sources": [],
                    "processing_time": time.time() - start_time,
                    "method": "hybrid_filtered_rag",
                    "stats": {
                        "titles_found": len(relevant_titles),
                        "chunks_filtered": len(filtered_ids),
                        "chunks_retrieved": 0
                    }
                }
            
            # Step 4: Format context and generate answer
            context = self.format_context(search_results)
            answer = self.generate_answer(query, context)
            
            # Format sources
            sources = []
            for metadata, distance in zip(
                search_results['metadatas'][0],
                search_results['distances'][0]
            ):
                chunk_info = None
                if metadata.get('is_chunked'):
                    chunk_info = f"Del {metadata['chunk_position']}"
                
                sources.append({
                    "title": metadata['title'],
                    "sfs_number": metadata.get('sfs_number', 'N/A'),
                    "url": metadata.get('url', 'N/A'),
                    "source_link": metadata.get('source_link', 'N/A'),
                    "similarity_score": round(1 - distance, 3),
                    "chunk_info": chunk_info
                })
            
            processing_time = time.time() - start_time
            
            print(f"\nüéâ Hybrid Retrieval Complete!")
            print(f"‚è±Ô∏è  Processing time: {processing_time:.2f} seconds")
            print(f"üìä Stats:")
            print(f"   - Relevant law titles found: {len(relevant_titles)}")
            print(f"   - Chunks filtered from DB: {len(filtered_ids)}")
            print(f"   - Top chunks retrieved: {len(sources)}")
            print("=" * 80)
            
            return {
                "answer": answer,
                "sources": sources,
                "processing_time": processing_time,
                "method": "hybrid_filtered_rag",
                "stats": {
                    "titles_found": len(relevant_titles),
                    "chunks_filtered": len(filtered_ids),
                    "chunks_retrieved": len(sources),
                    "relevant_laws": relevant_titles
                }
            }
            
        except Exception as e:
            error_msg = f"Ett fel uppstod i hybrid retrieval: {e}"
            print(f"‚ùå {error_msg}")
            return {
                "answer": error_msg,
                "sources": [],
                "processing_time": time.time() - start_time,
                "method": "hybrid_filtered_rag"
            }
def test_interactive():
    """Interactive test mode - allows user to input their own prompts"""
    print("Swedish Laws Hybrid Retrieval System - Interactive Mode")
    print("=" * 80)
    print("This system combines:")
    print("1. AI-based title filtering")
    print("2. ChromaDB filtered RAG")
    print("3. Gemini answer generation")
    print("=" * 80)
    
    # Initialize system
    try:
        retrieval_system = HybridLegalRetrieval()
        print("‚úÖ System initialized successfully!")
    except Exception as e:
        print(f"‚ùå Failed to initialize: {e}")
        return
    
    print("\nType your legal questions in Swedish. Type 'quit' or 'exit' to stop.")
    print("-" * 80)
    
    while True:
        try:
            query = input("\nüîç Your question: ").strip()
            
            if query.lower() in ['quit', 'exit', 'q']:
                print("üëã Goodbye!")
                break
            
            if not query:
                print("Please enter a valid question.")
                continue
            
            print(f"\n{'='*100}")
            print(f"PROCESSING: {query}")
            print('='*100)
            
            # Process the query
            result = retrieval_system.process_query(query, top_k=3)
            
            print(f"\nüí¨ ANSWER:")
            print("-" * 80)
            print(result['answer'])
            
            print(f"\nüîó SOURCES ({len(result['sources'])}):")
            print("-" * 80)
            for j, source in enumerate(result['sources'], 1):
                chunk_info = f" [{source['chunk_info']}]" if source['chunk_info'] else ""
                print(f"{j}. {source['title']}{chunk_info}")
                print(f"   Similarity: {source['similarity_score']:.3f}")
                print(f"   URL: {source['url']}")
            
            print(f"\nüìä PROCESS DETAILS:")
            print("-" * 80)
            if 'stats' in result:
                print(f"Relevant law titles found: {result['stats']['titles_found']}")
                print(f"Chunks filtered: {result['stats']['chunks_filtered']}")
                print(f"Chunks retrieved: {result['stats']['chunks_retrieved']}")
            print(f"Processing time: {result['processing_time']:.2f}s")
        except KeyboardInterrupt:
            print("\nüëã Goodbye!")
            break
        except Exception as e:
            print(f"‚ùå Error processing query: {e}")

def main():
    """Test the hybrid retrieval system"""
    print("Swedish Laws Hybrid Retrieval System Test")
    print("=" * 80)
    # Initialize system
    try:
        retrieval_system = HybridLegalRetrieval()
    except Exception as e:
        print(f"‚ùå Failed to initialize: {e}")
        return
    
    # Test queries
    test_queries = [
        "Vad s√§ger lagen om skatt p√• naturgrus?",
        "Vad s√§ger lagen om diskriminering av deltidsanst√§llda?",
    ]
    
    for i, query in enumerate(test_queries, 1):
        print(f"\n\n{'='*100}")
        print(f"TEST {i}: {query}")
        print('='*100)
        
        result = retrieval_system.process_query(query, top_k=10)
        
        print(f"\nüí¨ ANSWER:")
        print("-" * 80)
        print(result['answer'])
        
        print(f"\nüîó SOURCES ({len(result['sources'])}):")
        print("-" * 80)
        for j, source in enumerate(result['sources'], 1):
            chunk_info = f" [{source['chunk_info']}]" if source['chunk_info'] else ""
            print(f"{j}. {source['title']}{chunk_info}")
            print(f"   Similarity: {source['similarity_score']:.3f}")
    
            print(f"   URL: {source['url']}")


if __name__ == "__main__":

    import sys
    if len(sys.argv) > 1 and sys.argv[1] == "--interactive":
        test_interactive()
    else:
        main()